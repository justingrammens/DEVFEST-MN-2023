{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain: Let's Start With The Basics\n",
    "\n",
    "This is a Python notebook. It allows us to run code segments and see the results. There's a series of imports that we'll do first and then load the models from OpenAI\n",
    "\n",
    "[LangChain Providers](https://python.langchain.com/docs/integrations/providers)\n",
    "\n",
    "NOTE: You will need to have an environment variable of OPENAI_API_KEY with your key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
    "             temperature=1,\n",
    "             max_tokens = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How much wood would a wood chuck chuck?\"\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "Passing in text is limiting as it requires the context to be continued be passed in. Enter the first concepts of working with LLMs and that is the idea of a prompt template. We define these templates with variables that we can then pass in to fill in the template before it's passed to the LLM to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "restaurant_template = \"\"\"\n",
    "I want you to act as a naming consultant for new restaurants.\n",
    "\n",
    "Return a list of restaurant names. Each name should be short, catchy and easy to remember.\n",
    "It should relate to the type of restaurant you are naming.\n",
    "\n",
    "What are some good names for a restaurant that is {restaurant_description}?\n",
    "\"\"\"\n",
    "\n",
    "# An example prompt with one input variable\n",
    "prompt_template = PromptTemplate(input_variables=[\"restaurant_description\"], template=restaurant_template)\n",
    "\n",
    "# we create our first chain! This uses the llm and also allows us to pass in a prompt template\n",
    "chain = LLMChain(llm=llm,\n",
    "                 prompt=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description1 = \"a chinese place that serves dim sum and other specialties\"\n",
    "description2 = \"an american place that is themed with baseball memorabilia\"\n",
    "description3 = \"a cafe that has live hard rock music and memorabilia\"\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(description1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero and Few Shot Learning\n",
    "\n",
    "Notice that in the last example we didn't really prime the model with any examples of the output. This is called zero-shot prompting. We just sent in some information to the LLM and let it go. There's also a way for us to give it a few examples and then ask it to generate response. See this example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# First, create the list of few shot examples.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we specify the template to format the examples we have provided.\n",
    "# We use the `PromptTemplate` class for this.\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can now generate a prompt using the `format` method.\n",
    "print(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
